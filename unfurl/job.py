# Copyright (c) 2020 Adam Souzis
# SPDX-License-Identifier: MIT
"""
A Job is generated by comparing a list of specs with the last known state of the system.
Job runs tasks, each of which has a configuration spec that is executed on the running system
Each task tracks and records its modifications to the system's state
"""

import collections
import types
import itertools
import os
import json
from .support import Status, Priority, Defaults, AttributeManager, Reason, NodeState
from .result import serialize_value, ChangeRecord
from .util import UnfurlError, UnfurlTaskError, to_enum
from .merge import merge_dicts
from .runtime import OperationalInstance
from . import logs as unfurl_logging
from .configurator import (
    TaskView,
    ConfiguratorResult,
)
from .planrequests import (
    PlanRequest,
    TaskRequest,
    TaskRequestGroup,
    SetStateRequest,
    JobRequest,
    do_render_requests,
    get_render_requests,
    set_fulfilled,
)
from .plan import Plan
from .localenv import LocalEnv

# note: need to import configurators even though it is unused
from . import display

try:
    from time import perf_counter
except ImportError:
    from time import clock as perf_counter
import logging

logger = logging.getLogger("unfurl")


class ConfigChange(OperationalInstance, ChangeRecord):
    """
    Represents a configuration change made to the system.
    It has a operating status and a list of dependencies that contribute to its status.
    There are two kinds of dependencies:

    1. Live resource attributes that the configuration's inputs depend on.
    2. Other configurations and resources it relies on to function properly.
    """

    def __init__(
        self, parentJob=None, startTime=None, status=None, previousId=None, **kw
    ):
        OperationalInstance.__init__(self, status, **kw)
        if parentJob:  # use the parent's job id and startTime
            ChangeRecord.__init__(self, parentJob.changeId, parentJob.startTime)
        else:  # generate a new job id and use the given startTime
            ChangeRecord.__init__(self, startTime=startTime, previousId=previousId)


class JobOptions(object):
    """
    Options available to select which tasks are run, e.g. read-only

    does the config apply to the operation?
    is it out of date?
    is it in a ok state?
    """

    defaults = dict(
        parentJob=None,
        startTime=None,
        out=None,
        verbose=0,
        instance=None,
        instances=None,
        template=None,
        useConfigurator=False,
        # default options:
        add=True,  # add new templates
        update=True,  # run configurations that whose spec has changed but don't require a major version change
        repair="error",  # or 'degraded' or "missing" or "none", run configurations that are not operational and/or degraded
        upgrade=False,  # run configurations with major version changes or whose spec has changed
        force=False,  # (re)run operation regardless of instance's status or state
        verify=False,  # XXX3 discover first and set status if it differs from expected state
        readonly=False,  # only run configurations that won't alter the system
        check=False,  # if new instances exist before deploying
        dryrun=False,
        planOnly=False,
        requiredOnly=False,
        prune=False,
        destroyunmanaged=False,
        append=None,
        replace=None,
        commit=False,
        dirty="auto",  # run the job even if the repository has uncommitted changrs
        message=None,
        workflow=Defaults.workflow,
    )

    def __init__(self, **kw):
        options = self.defaults.copy()
        options["instance"] = kw.get("resource")  # old option name
        if kw.get("starttime"):
            options["startTime"] = kw["starttime"]
        options.update(kw)
        self.__dict__.update(options)
        self.userConfig = kw

    def get_user_settings(self):
        # only include settings different from the defaults
        return {
            k: self.userConfig[k]
            for k in set(self.userConfig) & set(self.defaults)
            if k != "out" and self.userConfig[k] != self.defaults[k]
        }


class ConfigTask(ConfigChange, TaskView):
    """
    receives a configSpec and a target node instance
    instantiates and runs Configurator
    updates Configurator's target's status and lastConfigChange
    """

    def __init__(self, job, configSpec, target, reason=None):
        ConfigChange.__init__(self, job)
        TaskView.__init__(self, job.runner.manifest, configSpec, target, reason)
        self.dry_run = job.dry_run
        self.verbose = job.verbose
        self._configurator = None
        self.generator = None
        self.job = job
        self.changeList = []
        self.result = None
        self.outputs = None
        # for summary:
        self.modified_target = False
        self.target_status = target.status
        self.target_state = target.state

        # set the attribute manager on the root resource
        self._attributeManager = AttributeManager(self._manifest.yaml)
        self.target.root.attributeManager = self._attributeManager

    @property
    def status(self):
        return self.local_status

    def priority():
        doc = "The priority property."

        def fget(self):
            if self._priority is None:
                return self.configSpec.should_run()
            else:
                return self._priority

        def fset(self, value):
            self._priority = value

        def fdel(self):
            del self._priority

        return locals()

    priority = property(**priority())

    @property
    def configurator(self):
        if self._configurator is None:
            self._configurator = self.configSpec.create()
        return self._configurator

    def start_run(self):
        self.generator = self.configurator.get_generator(self)
        assert isinstance(self.generator, types.GeneratorType)

    def send(self, change):
        result = None
        try:
            result = self.generator.send(change)
        finally:
            # serialize configuration changes
            self.commit_changes()
        return result

    def start(self):
        self.start_run()
        self.target.root.attributeManager = self._attributeManager
        self.target_status = self.target.status
        self.target_state = self.target.state

    def _update_status(self, result):
        """
        Update the instances status with the result of the operation.
        If status wasn't explicitly set but the operation changed the instance's configuration
        or state, choose a status based on the type of operation.
        """
        if result.status is not None:
            # status was explicitly set
            self.target.local_status = result.status
            if self.target.present and self.target.created is None:
                if self.configSpec.operation not in [
                    "check",
                    "discover",
                ]:
                    self.target.created = self.changeId
            return True
        elif not result.success:
            # if any task failed and (maybe) modified, target.status will be set to error or unknown
            if result.modified:
                self.target.local_status = (
                    Status.error if self.required else Status.degraded
                )
                return True
            elif result.modified is None:
                self.target.local_status = Status.unknown
                return True
            # otherwise doesn't modify target status
        return False

    def _update_last_change(self, result):
        """
        If the target's configuration or state has changed, set the instance's lastChange
        state to this tasks' changeid.
        """
        if self.target.last_change is None and self.target.status != Status.pending:
            # hacky but always save _lastConfigChange the first time to
            # distinguish this from a brand new resource
            self.target._lastConfigChange = self.changeId
            return True
        if result.modified or self._resourceChanges.get_attribute_changes(
            self.target.key
        ):
            if self.target.last_change != self.changeId:
                # save to create a linked list of tasks that modified the target
                self.previousId = self.target.last_change
            self.target._lastStateChange = self.changeId
            return True
        return False

    def finished_workflow(self, successStatus):
        instance = self.target
        if instance.local_status == successStatus:
            return  # hasn't changed
        self.modified_target = True
        instance.local_status = successStatus
        self.target_status = successStatus
        if instance.last_change != self.changeId:
            # save to create a linked list of tasks that modified the target
            self.previousId = instance.last_change
        instance._lastConfigChange = self.changeId
        if successStatus == Status.ok and instance.created is None:
            instance.created = self.changeId

    def finished(self, result):
        assert result
        if self.generator:
            self.generator.close()
            self.generator = None

        self.outputs = result.outputs

        # don't set the changeId until we're finish so that we have a higher changeid
        # than nested tasks and jobs that ran
        # (task that never run will have the same changeId as its parent)
        self.set_task_id(self.job.runner.increment_task_count())
        # XXX2 if attributes changed validate using attributesSchema
        # XXX2 Check that configuration provided the metadata that it declared (check postCondition)

        if self.changeList:
            # merge changes together (will be saved with changeset)
            changes = self.changeList
            accum = changes.pop(0)
            while changes:
                accum = merge_dicts(accum, changes.pop(0))

            # note: this might set _lastConfigChange on instances other than this target
            self._resourceChanges.update_changes(
                accum, self._attributeManager.statuses, self.target, self.changeId
            )
            # XXX implement:
            # if not result.applied:
            #    self._resourceChanges.rollback(self.target)

        # now that resourceChanges finalized:
        self._update_status(result)
        targetChanged = self._update_last_change(result)
        self.result = result
        self.local_status = Status.ok if result.success else Status.error
        self.modified_target = targetChanged or self.target_status != self.target.status
        self.target_status = self.target.status
        self.target_state = self.target.state
        return self

    def commit_changes(self):
        """
        This can be called multiple times if the configurator yields multiple times.
        Save the changes made each time.
        """
        changes, liveDependencies = self._attributeManager.commit_changes()
        self.changeList.append(changes)
        # record the live attributes that we are dependent on
        for key, (target, attributes) in liveDependencies.items():
            if target is not self.target:
                for name, value in attributes.items():
                    self.add_dependency(key + "::" + name, value, target=target)
        return changes, liveDependencies

    # unused
    # def findLastConfigureOperation(self):
    #     if not self._manifest.changeSets:
    #         return None
    #     previousId = self.target.lastChange
    #     while previousId:
    #         previousChange = self._manifest.changeSets.get(previousId)
    #         if not previousChange:
    #             return None
    #         if previousChange.target != self.target.key:
    #             return (
    #                 None  # XXX handle case where lastChange was set by another target
    #             )
    #         if previousChange.operation == self.configSpec.operation:
    #             return previousChange
    #         previousId = previousChange.previousId
    #     return None

    def has_inputs_changed(self):
        """
        Evaluate configuration spec's inputs and compare with the current inputs' values
        """
        changeset = self._manifest.find_last_operation(
            self.target.key, self.configSpec.operation
        )
        if not changeset:
            return False

        return self.configurator.check_digest(self, changeset)

    def has_dependencies_changed(self):
        return False
        # XXX artifacts
        # XXX identity of requirements (how? what about imported nodes? instance keys?)
        # dynamic dependencies:
        # return any(d.hasChanged(self) for d in self.dependencies)

    # XXX unused
    # def refreshDependencies(self):
    #     for d in self.dependencies:
    #         d.refresh(self)

    @property
    def name(self):
        name = self.configSpec.name
        if self.configSpec.operation and self.configSpec.operation not in name:
            name = name + ":" + self.configSpec.operation
        if self.reason and self.reason not in name:
            return name + ":" + self.reason
        return name

    def summary(self, asJson=False):
        if self.target.name != self.target.template.name:
            rname = "%s (%s)" % (self.target.name, self.target.template.name)
        else:
            rname = self.target.name

        if self.configSpec.name != self.configSpec.className:
            cname = "%s (%s)" % (self.configSpec.name, self.configSpec.className)
        else:
            cname = self.configSpec.name

        if self._configurator:
            cClass = self._configurator.__class__
            configurator = "%s.%s" % (cClass.__module__, cClass.__name__)
        else:
            configurator = self.configSpec.className

        summary = dict(
            status=self.status.name,
            target=self.target.name,
            operation=self.configSpec.operation,
            template=self.target.template.name,
            type=self.target.template.type,
            targetStatus=self.target_status.name,
            targetState=self.target_state and self.target_state.name or None,
            changed=self.modified_target,
            configurator=configurator,
            priority=self.priority.name,
            reason=self.reason or "",
        )

        if asJson:
            return summary
        else:
            return (
                "{operation} on instance {rname} (type {type}, status {targetStatus}) "
                + "using configurator {cname}, priority: {priority}, reason: {reason}"
            ).format(cname=cname, rname=rname, **summary)

    def __repr__(self):
        return "ConfigTask(%s:%s)" % (self.target, self.name)


class Job(ConfigChange):
    """
    runs ConfigTasks and child Jobs
    """

    MAX_NESTED_SUBTASKS = 100

    def __init__(self, runner, rootResource, jobOptions, previousId=None):
        assert isinstance(jobOptions, JobOptions)
        self.__dict__.update(jobOptions.__dict__)
        super(Job, self).__init__(self.parentJob, self.startTime, Status.ok, previousId)
        self.dry_run = jobOptions.dryrun

        self.jobOptions = jobOptions
        self.runner = runner
        self.rootResource = rootResource
        self.jobRequestQueue = []
        self.unexpectedAbort = None
        self.workDone = collections.OrderedDict()
        self.timeElapsed = 0

    def get_operational_dependencies(self):
        # XXX3 this isn't right, root job might have too many and child job might not have enough
        # plus dynamic configurations probably shouldn't be included if yielded by a configurator
        for task in self.workDone.values():
            yield task

    def get_outputs(self):
        return self.rootResource.outputs.attributes

    def run_query(self, query, trace=0):
        from .eval import eval_for_func, RefContext

        return eval_for_func(query, RefContext(self.rootResource, trace=trace))

    def create_task(self, configSpec, target, reason=None):
        # XXX2 if operation_host set, create remote task instead
        task = ConfigTask(self, configSpec, target, reason=reason)
        try:
            task.inputs
            task.configurator
        except Exception:
            UnfurlTaskError(task, "unable to create task")

        # if configSpec.hasBatchConfigurator():
        # search targets parents for a batchConfigurator
        # XXX how to associate a batchConfigurator with a resource and when is its task created?
        # batchConfigurator tasks more like a job because they have multiple changeids
        #  batchConfiguratorJob = findBatchConfigurator(configSpec, target)
        #  batchConfiguratorJob.add(task)
        #  return None

        return task

    def validate_job_options(self):
        if self.jobOptions.instance and not self.rootResource.find_resource(
            self.jobOptions.instance
        ):
            logger.warning(
                'selected instance not found: "%s"', self.jobOptions.instance
            )

    def run(self):
        ready = self.create_plan()
        self.workDone = collections.OrderedDict()
        notReady = []
        while ready:
            # first time render them all, after that only re-render requests if their dependencies were fulfilled
            ready, unfulfilled, errors = do_render_requests(self, ready)
            notReady.extend(unfulfilled)
            self.apply(ready)  # XXX what about errors?
            ready, notReady = set_fulfilled(notReady, ready)

        # if there were circular dependencies or errors there will be notReady won't be empty
        if notReady:
            for parent, req in get_render_requests(notReady):
                message = "can't fulfill %s: never ran %s" % (
                    req.target.name,
                    req.future_dependencies,
                )
                logger.debug(message)
                req.task.finished(ConfiguratorResult(False, False, result=message))

        # the jobRequestQueue will have jobs that were added dynamically by a configurator
        # but were not yielding inside runTask
        while self.jobRequestQueue:
            jobRequest = self.jobRequestQueue[0]
            job = self.run_job_request(jobRequest)
            if self.should_abort(job):
                return self.rootResource

        return self.rootResource

    def get_candidate_task_requests(self):
        planGen = self.plan.execute_plan()
        result = None
        try:
            while True:
                req = planGen.send(result)
                result = yield req
        except StopIteration:
            pass

    def create_plan(self):
        self.validate_job_options()
        self.taskRequests = list(self.get_candidate_task_requests())
        return self.taskRequests

    def _get_success_status(self, workflow, success):
        if isinstance(success, Status):
            return success
        if success:
            return self.plan.get_success_status(workflow)
        return None

    def apply_group(self, depth, groupRequest):
        workflow = groupRequest.workflow
        task, success = self.apply(groupRequest.children, depth, groupRequest)
        if task:
            successStatus = self._get_success_status(workflow, success)
            # logging.debug("successStatus %s for %s", task.target.state, workflow)
            if successStatus is not None:
                # one of the child tasks succeeded and the workflow is one that modifies the target
                # update the target's status
                task.finished_workflow(successStatus)
        return task

    def apply(self, taskRequests, depth=0, parent=None):
        failed = False
        task = None
        successStatus = False
        workflow = parent and parent.workflow or None
        for taskRequest in taskRequests:
            # if parent is set, stop processing requests once one fails
            if parent and failed:
                logger.debug(
                    "Skipping task %s because previous operation failed", taskRequest
                )
                continue
            logger.info("Running task %s", taskRequest)

            if isinstance(taskRequest, TaskRequestGroup):
                _task = self.apply_group(depth, taskRequest)
            else:
                _task = self._run_operation(taskRequest, workflow, depth)
            if not _task:
                continue
            task = _task

            if task.result.success:
                if parent and task.target is parent.target:
                    # if the task explicitly set the status use that
                    if task.result.status is not None:
                        successStatus = task.result.status
                    else:
                        successStatus = True
            else:
                failed = True
        return task, successStatus

    def run_job_request(self, jobRequest):
        logger.debug("running jobrequest: %s", jobRequest)
        self.jobRequestQueue.remove(jobRequest)
        resourceNames = [r.name for r in jobRequest.instances]
        jobOptions = JobOptions(
            parentJob=self, repair="missing", instances=resourceNames
        )
        childJob = self.runner.create_job(jobOptions)
        childJob.set_task_id(self.runner.increment_task_count())
        assert childJob.parentJob is self
        childJob.run()
        return childJob

    def _dependency_check(self, instance):
        dependencies = list(instance.get_operational_dependencies())
        missing = [dep for dep in dependencies if not dep.operational and dep.required]
        if missing:
            reason = "required dependencies not operational: %s" % ", ".join(
                ["%s is %s" % (dep.name, dep.status.name) for dep in missing]
            )
        else:
            reason = ""
        return missing, reason

    def should_run_task(self, task):
        """
        Checked at runtime right before each task is run
        """
        try:
            if task._configurator:
                priority = task.configurator.should_run(task)
            else:
                priority = task.priority
        except Exception:
            # unexpected error don't run this
            UnfurlTaskError(task, "shouldRun failed unexpectedly")
            return False, "unexpected failure"

        if isinstance(priority, bool):
            priority = priority and Priority.required or Priority.ignore
        else:
            priority = to_enum(Priority, priority)
        if priority != task.priority:
            logger.debug(
                "configurator changed task %s priority from %s to %s",
                task,
                task.priority,
                priority,
            )
            task.priority = priority
        if not priority > Priority.ignore:
            return False, "configurator cancelled"

        if task.reason == Reason.reconfigure:
            if task.has_inputs_changed() or task.has_dependencies_changed():
                return True, "change detected"
            else:
                return False, "no change detected"

        return True, "proceed"

    def can_run_task(self, task):
        """
        Checked at runtime right before each task is run

        * validate inputs
        * check pre-conditions to see if it can be run
        * check task if it can be run
        """
        can_run = False
        reason = ""
        try:
            if task._errors:
                can_run = False
                reason = "could not create task"
            elif task.dry_run and not task.configurator.can_dry_run(task):
                can_run = False
                reason = "dry run not supported"
            else:
                missing = []
                if task.configSpec.operation != "check":
                    missing, reason = self._dependency_check(task.target)
                if not missing:
                    missing, reason = self._dependency_check(task)
                if not missing:
                    errors = task.configSpec.find_invalidate_inputs(task.inputs)
                    if errors:
                        reason = "invalid inputs: %s" % str(errors)
                    else:
                        preErrors = task.configSpec.find_invalid_preconditions(
                            task.target
                        )
                        if preErrors:
                            reason = "invalid preconditions: %s" % str(preErrors)
                        else:
                            errors = task.configurator.can_run(task)
                            if not errors or not isinstance(errors, bool):
                                reason = "configurator declined: %s" % str(errors)
                            else:
                                can_run = True
        except Exception:
            UnfurlTaskError(task, "cantRunTask failed unexpectedly")
            reason = "unexpected exception in cantRunTask"
            can_run = False

        if can_run:
            return True, ""
        else:
            logger.info("could not run task %s: %s", task, reason)
            return False, "could not run: " + reason

    def should_abort(self, task):
        return False  # XXX3

    def _set_state(self, req):
        logger.debug("setting state %s for %s", req.set_state, req.target)
        resource = req.target
        if "managed" in req.set_state:
            resource.created = False if req.set_state == "unmanaged" else self.changeId
        else:
            try:
                resource.state = req.set_state
            except KeyError:
                resource.local_status = to_enum(Status, req.set_state)

    def _entry_test(self, req, workflow):
        """
        Operations can dynamically advance the state of a instance so that an operation
        added by the plan no longer needs to run.
        For example, a check operation may determine a resource is already created
        or a create operation may also configure and start an instance.
        """
        resource = req.target
        logger.trace(
            "checking operation entry test: current state %s start state %s op %s workflow %s",
            resource.state,
            req.startState,
            req.configSpec.operation,
            workflow,
        )
        if req.configSpec.operation == "check":
            missing, reason = self._dependency_check(resource)
            if missing:
                return False, reason
            if not workflow:
                if (
                    self.is_change_id(resource.parent.created)
                    and self.get_job_id(resource.parent.created) == self.changeId
                ):
                    # optimization:
                    # if parent was created during this job we don't need to run check operation
                    # we know we couldn't have existed
                    resource._localStatus = Status.pending
                    return False, "skipping check on a new instance"
                else:
                    return True, "passed"

        if self.jobOptions.force:  # always run
            return True, "passed"

        if self.plan.get_success_status(workflow) == resource.status:
            return False, "instance already has desired status"

        if req.startState and resource.state and workflow == "deploy":
            # if we set a startState and force isn't set then only run
            # if we haven't already advanced to that state by another operation
            entryTest = NodeState(req.startState + 1)
            if resource.state > NodeState.started:
                # "started" is the final deploy state, target state must be stopped or deleted or error
                if req.configSpec.operation == "start":
                    # start operations can't handle deleted nodes
                    return (
                        resource.state <= NodeState.stopped,
                        "can't start a missing instance",
                    )
            elif resource.state > entryTest:
                return False, "instance already entered state"
        return True, "passed"

    def _run_operation(self, req, workflow, depth):
        if isinstance(req, SetStateRequest):
            self._set_state(req)
            return None
        assert isinstance(req, TaskRequest)
        if req.error:
            return None

        test, msg = self._entry_test(req, workflow)
        if not test:
            logger.debug(
                "skipping operation %s for instance %s with state %s and status %s: %s",
                req.configSpec.operation,
                req.target.name,
                req.target.state,
                req.target.status,
                msg,
            )
            return None

        task = req.task or self.create_task(
            req.configSpec, req.target, reason=req.reason
        )
        if task:
            proceed, msg = self.should_run_task(task)
            if not proceed:
                logger.debug(
                    "skipping task %s for instance %s with state %s and status %s: %s",
                    req.configSpec.operation,
                    req.target.name,
                    req.target.state,
                    req.target.status,
                    msg,
                )
                return None
            resource = req.target
            startingStatus = resource._localStatus
            if req.startState is not None:
                resource.state = req.startState
            startingState = resource.state
            self.runner.add_work(task)
            self.run_task(task, depth)

            # if # task succeeded but didn't update nodestate
            if task.result.success and resource.state == startingState:
                if req.startState is not None:
                    # advance the state if a startState was set in the TaskRequest
                    resource.state = NodeState(req.startState + 1)
                elif (
                    req.configSpec.operation == "check"
                    and startingStatus != resource._localStatus
                ):
                    # if check operation set status but didn't update state, set a default state
                    state = {
                        Status.ok: NodeState.started,
                        Status.error: NodeState.error,
                        Status.absent: NodeState.deleted,
                        Status.pending: NodeState.initial,
                    }.get(resource._localStatus)
                    if state is not None:
                        resource.state = state
                task.target_state = resource.state

            # logger.debug(
            #     "changed %s to %s, wanted %s",
            #     startingState,
            #     task.target.state,
            #     req.startState,
            # )
            logger.info(
                "finished taskrequest %s: %s %s %s",
                task,
                "success" if task.result.success else "failed",
                task.target.status.name,
                task.target.state and task.target.state.name or "",
            )

        return task

    def run_task(self, task, depth=0):
        """
        During each task run:
        * Notification of metadata changes that reflect changes made to resources
        * Notification of add or removing dependency on a resource or properties of a resource
        * Notification of creation or deletion of a resource
        * Requests a resource with requested metadata, if it doesn't exist, a task is run to make it so
        (e.g. add a dns entry, install a package).

        Returns a task.
        """
        ok, errors = self.can_run_task(task)
        if not ok:
            return task.finished(ConfiguratorResult(False, False, result=errors))

        task.start()
        change = None
        while True:
            try:
                result = task.send(change)
            except Exception:
                UnfurlTaskError(task, "configurator.run failed")
                return task.finished(ConfiguratorResult(False, None, Status.error))
            if isinstance(result, PlanRequest):
                if depth >= self.MAX_NESTED_SUBTASKS:
                    UnfurlTaskError(task, "too many subtasks spawned")
                    change = task.finished(ConfiguratorResult(False, None))
                else:
                    ready, _, errors = do_render_requests(self, [result])
                    if not ready:
                        return result.task.finished(
                            ConfiguratorResult(False, False, result=errors)
                        )
                    change, success = self.apply(ready, depth + 1)
            elif isinstance(result, JobRequest):
                job = self.run_job_request(result)
                change = job
            elif isinstance(result, ConfiguratorResult):
                retVal = task.finished(result)
                logger.debug(
                    "completed task %s: %s; %s", task, task.target.status, result
                )
                return retVal
            else:
                UnfurlTaskError(task, "unexpected result from configurator")
                return task.finished(ConfiguratorResult(False, None, Status.error))

    ###########################################################################
    ### Reporting methods
    ###########################################################################
    def _json_plan_summary(self, pretty=False):
        """
        plan = instance+,
        [
          {
          instance
          plan: [
              {"operation": "check"
                "sequence": [
                    <plan>,
                  ]
              }
            ]
          }
        ]
        """

        def _summary(requests, target, parent):
            children = parent
            for request in requests:
                isGroup = isinstance(request, TaskRequestGroup)
                if isGroup and not request.children:
                    continue  # don't include in the plan
                if request.target is not target:
                    target = request.target
                    children = []
                    node = dict(
                        instance=target.name,
                        status=str(target.status),
                        state=str(target.state),
                        managed=target.created,
                        plan=children,
                    )
                    parent.append(node)
                if isGroup:
                    sequence = []
                    group = {}
                    if request.workflow:
                        group["workflow"] = str(request.workflow)
                    group["sequence"] = sequence
                    children.append(group)
                    _summary(request.children, target, sequence)
                else:
                    children.append(request._summary_dict())

        summary = []
        _summary(self.taskRequests, None, summary)
        if not pretty:
            return summary
        else:
            return json.dumps(summary, indent=2)

    def json_summary(self, pretty=False):
        if self.jobOptions.planOnly:
            return self._json_plan_summary(pretty)

        job = dict(id=self.changeId, status=self.status.name)
        job.update(self.stats())
        if not self.startTime:  # skip if startTime was explicitly set
            job["timeElapsed"] = self.timeElapsed
        summary = dict(
            job=job,
            outputs=serialize_value(self.get_outputs()),
            tasks=[task.summary(True) for task in self.workDone.values()],
        )
        if pretty:
            return json.dumps(summary, indent=2)
        return summary

    def stats(self, asMessage=False):
        tasks = self.workDone.values()
        key = lambda t: t._localStatus or Status.unknown
        tasks = sorted(tasks, key=key)
        stats = dict(total=len(tasks), ok=0, error=0, unknown=0, skipped=0)
        for k, g in itertools.groupby(tasks, key):
            if not k:  # is a Status
                stats["skipped"] = len(list(g))
            else:
                stats[k.name] = len(list(g))
        stats["changed"] = len([t for t in tasks if t.modified_target])
        if asMessage:
            return "{total} tasks ({changed} changed, {ok} ok, {error} failed, {unknown} unknown, {skipped} skipped)".format(
                **stats
            )
        return stats

    def _plan_summary(self):
        """
        Node "site" (status, state, created):
          check: Install.check
          workflow: # if group
            Standard.create (reason add)
            Standard.configure (reason add)
        """

        def _summary(requests, target, indent):
            for request in requests:
                isGroup = isinstance(request, TaskRequestGroup)
                if isGroup and not request.children:
                    continue
                if request.target is not target:
                    target = request.target
                    status = ", ".join(
                        filter(
                            None,
                            (
                                target.status.name if target.status is not None else "",
                                target.state.name if target.state is not None else "",
                                "managed" if target.created else "",
                            ),
                        )
                    )
                    nodeStr = 'Node "%s" (%s):' % (target.name, status)
                    output.append(" " * indent + nodeStr)
                if isGroup:
                    output.append(
                        "%s- %s:" % (" " * indent, (request.workflow or "sequence"))
                    )
                    _summary(request.children, target, indent + 4)
                else:
                    output.append(" " * indent + "- " + request.name)

        opts = self.jobOptions.get_user_settings()
        options = ",".join(["%s = %s" % (k, opts[k]) for k in opts if k != "planOnly"])
        header = "Plan for %s" % self.workflow
        if options:
            header += " (%s)" % options
        output = [header + ":\n"]
        _summary(self.taskRequests, None, 0)
        if len(output) <= 1:
            output.append("Nothing to do.")
        return "\n".join(output)

    def summary(self):
        if self.jobOptions.planOnly:
            return self._plan_summary()

        outputString = ""
        outputs = self.get_outputs()
        if outputs:
            outputString = "\nOutputs:\n    " + "\n    ".join(
                "%s: %s" % (name, value)
                for name, value in serialize_value(outputs).items()
            )

        if not self.workDone:
            return "Job %s completed: %s. Found nothing to do. %s" % (
                self.changeId,
                self.status.name,
                outputString,
            )

        def format(i, task):
            return "%d. %s; %s" % (i, task.summary(), task.result or "skipped")

        line1 = "Job %s completed in %.3fs: %s. %s:\n    " % (
            self.changeId,
            self.timeElapsed,
            self.status.name,
            self.stats(asMessage=True),
        )
        tasks = "\n    ".join(
            format(i + 1, task) for i, task in enumerate(self.workDone.values())
        )
        return line1 + tasks + outputString


class Runner(object):
    def __init__(self, manifest):
        self.manifest = manifest
        assert self.manifest.tosca
        self.taskCount = 0
        self.currentJob = None

    def add_work(self, task):
        key = id(task)
        self.currentJob.workDone[key] = task
        task.job.workDone[key] = task

    def create_job(self, joboptions, previousId=None):
        """
        Selects task to run based on the workflow and job options
        """
        root = self.manifest.get_root_resource()
        assert self.manifest.tosca
        job = Job(self, root, joboptions, previousId)

        if (
            self.manifest.localEnv
            and not joboptions.parentJob
            and not joboptions.startTime
        ):
            logPath = self.manifest.get_job_log_path(job.get_start_time(), ".log")
            if not os.path.isdir(os.path.dirname(logPath)):
                os.makedirs(os.path.dirname(logPath))
            unfurl_logging.add_log_file(logPath)
            path = self.manifest.path
            if joboptions.planOnly:
                logger.info("creating %s plan for %s", joboptions.workflow, path)
            else:
                logger.info("starting %s job for %s", joboptions.workflow, path)

        WorkflowPlan = Plan.get_plan_class_for_workflow(joboptions.workflow)
        if not WorkflowPlan:
            raise UnfurlError("unknown workflow: %s" % joboptions.workflow)
        job.plan = WorkflowPlan(root, self.manifest.tosca, joboptions)
        return job

    def increment_task_count(self):
        self.taskCount += 1
        return self.taskCount

    def run(self, jobOptions=None):
        job = None
        try:
            cwd = os.getcwd()
            if self.manifest.get_base_dir():
                os.chdir(self.manifest.get_base_dir())
            if jobOptions is None:
                jobOptions = JobOptions()

            if jobOptions.dirty == "auto":  # default to false if committing
                checkIfClean = jobOptions.commit
            else:
                checkIfClean = jobOptions.dirty == "abort"
            if not jobOptions.planOnly and checkIfClean:
                for repo in self.manifest.repositories.values():
                    if repo.is_dirty():
                        logger.error(
                            "aborting run: uncommitted files in %s (--dirty=ok to override)",
                            repo.working_dir,
                        )
                        return None

            job = self.create_job(
                jobOptions, self.manifest.lastJob and self.manifest.lastJob["changeId"]
            )
            startTime = perf_counter()
            self.currentJob = job
            try:
                display.verbosity = jobOptions.verbose
                if jobOptions.planOnly:
                    job.create_plan()
                else:
                    job.run()
            except Exception:
                job.local_status = Status.error
                job.unexpectedAbort = UnfurlError(
                    "unexpected exception while running job", True, True
                )
            self.currentJob = None
            self.manifest.commit_job(job)
        finally:
            if job:
                job.timeElapsed = perf_counter() - startTime
            os.chdir(cwd)
        return job


def run_job(manifestPath=None, _opts=None):
    """
    Loads the given Ensemble and creates and runs a job.

    Args:
        manifestPath (:obj:`str`, optional) The path the Ensemble manifest.
         If None, it will look for an ensemble in the current working directory.
        _opts (:obj:`dict`, optional) A dictionary of job options. Names and values should match
          the names of the command line options for creating jobs.

    Returns:
        (:obj:`Job`): The job that just ran.
    """
    _opts = _opts or {}
    localEnv = LocalEnv(manifestPath, _opts.get("home"))
    opts = JobOptions(**_opts)
    path = localEnv.manifestPath
    if not opts.planOnly:
        logger.info("creating %s job for %s", opts.workflow, path)
    try:
        manifest = localEnv.get_manifest()
    except Exception as e:
        logger.error(
            "failed to load manifest at %s: %s",
            path,
            str(e),
            exc_info=opts.verbose >= 2,
        )
        return None

    runner = Runner(manifest)
    return runner.run(opts)
